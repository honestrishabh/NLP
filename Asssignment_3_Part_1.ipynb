{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW3FIs2MI7Zv"
      },
      "source": [
        "## **ASSIGNMENT 3 PART 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ1T379hP6FV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GcUns0KDIwm0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "# Load dataset\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "#Prepare data\n",
        "texts = df['review'].values\n",
        "labels = df['sentiment'].map({'positive': 1, 'negative': 0}).values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences\n",
        "maxlen = 100\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IGGiDoqP8ao",
        "outputId": "18e59f59-00ef-4c2f-cec4-9d075e15fce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-01 06:25:32--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-09-01 06:25:32--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-09-01 06:25:32--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.11MB/s    in 2m 39s  \n",
            "\n",
            "2024-09-01 06:28:11 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "# Download the GloVe embeddings (in your terminal or Jupyter notebook cell)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# Unzip the file\n",
        "!unzip glove.6B.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GwVizy7AJIEU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(file_path):\n",
        "    embedding_index = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = coefs\n",
        "    return embedding_index\n",
        "\n",
        "glove_file_path = 'glove.6B.100d.txt'\n",
        "embedding_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_dim = 100\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if word in embedding_index:\n",
        "        embedding_matrix[i] = embedding_index[word]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding matrix\n",
        "embedding_dim = 100\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(10000, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "# Make sure to execute the cell where 'embedding_index' is defined before running this cell\n",
        "for word, i in word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embedding_index.get(word) # Now you can access 'embedding_index'\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "MzmBmLzzYInd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hLVBUaNtPKKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a928b0c-7f72-41b3-95cc-df215e81c9f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 34ms/step - accuracy: 0.6143 - loss: 0.6471 - val_accuracy: 0.6846 - val_loss: 0.5965\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 41ms/step - accuracy: 0.6638 - loss: 0.6093 - val_accuracy: 0.6926 - val_loss: 0.5973\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 41ms/step - accuracy: 0.6494 - loss: 0.6243 - val_accuracy: 0.6046 - val_loss: 0.6569\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6669 - loss: 0.6104 - val_accuracy: 0.6938 - val_loss: 0.6535\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.6569 - loss: 0.6249 - val_accuracy: 0.7074 - val_loss: 0.5776\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b6dad402e30>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_4pQYoVYCC7",
        "outputId": "85b7539c-b900-49dd-fee7-cb9c93b6853a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 131ms/step - accuracy: 0.6833 - loss: 0.5725 - val_accuracy: 0.7935 - val_loss: 0.4395\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 128ms/step - accuracy: 0.8128 - loss: 0.4057 - val_accuracy: 0.8241 - val_loss: 0.3872\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 126ms/step - accuracy: 0.8419 - loss: 0.3579 - val_accuracy: 0.8434 - val_loss: 0.3472\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 132ms/step - accuracy: 0.8516 - loss: 0.3345 - val_accuracy: 0.8508 - val_loss: 0.3320\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 128ms/step - accuracy: 0.8656 - loss: 0.3111 - val_accuracy: 0.8510 - val_loss: 0.3287\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b6db5258910>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn import functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "# Convert to tensor\n",
        "X_train_tensor = torch.tensor(X_train_padded, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_padded, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n"
      ],
      "metadata": {
        "id": "tl-gG-idZNnd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "model = RNNModel(num_words, embedding_dim, 64)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in test_loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs.squeeze() > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f'Epoch {epoch + 1}, Accuracy: {correct / total}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aHYi99rZQnl",
        "outputId": "3c329145-32d4-454a-99ce-f7baa2fd0d8f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Accuracy: 0.7358\n",
            "Epoch 2, Accuracy: 0.7559\n",
            "Epoch 3, Accuracy: 0.7725\n",
            "Epoch 4, Accuracy: 0.6792\n",
            "Epoch 5, Accuracy: 0.7736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "model = LSTMModel(num_words, embedding_dim, 64)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in test_loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs.squeeze() > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f'Epoch {epoch + 1}, Accuracy: {correct / total}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9b2y5AlZUOx",
        "outputId": "bd67c84d-ef76-412f-b919-499edd7988f9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Accuracy: 0.8086\n",
            "Epoch 2, Accuracy: 0.8443\n",
            "Epoch 3, Accuracy: 0.8637\n",
            "Epoch 4, Accuracy: 0.864\n",
            "Epoch 5, Accuracy: 0.871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3njpEonbxJN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}